{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/preprocessed_cleaned_body.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Bias</th>\n",
       "      <th>cleaned_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abortion rights advocates have asked the U.S. ...</td>\n",
       "      <td>1.67</td>\n",
       "      <td>abortion right advocate asked u supreme court ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A federal appeals court rejected the most dire...</td>\n",
       "      <td>0.67</td>\n",
       "      <td>federal appeal court rejected direct constitut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As part of the Trump administration's effort t...</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>part trump administration effort slow migrant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>President Donald Trump and \"the Trump of the T...</td>\n",
       "      <td>-4.33</td>\n",
       "      <td>president donald trump trump tropic brazilian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S Senator Elizabeth Warren, who is competing...</td>\n",
       "      <td>-10.00</td>\n",
       "      <td>u senator elizabeth warren competing democrati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body   Bias  \\\n",
       "0  Abortion rights advocates have asked the U.S. ...   1.67   \n",
       "1  A federal appeals court rejected the most dire...   0.67   \n",
       "2  As part of the Trump administration's effort t...  -2.75   \n",
       "3  President Donald Trump and \"the Trump of the T...  -4.33   \n",
       "4  U.S Senator Elizabeth Warren, who is competing... -10.00   \n",
       "\n",
       "                                        cleaned_body  \n",
       "0  abortion right advocate asked u supreme court ...  \n",
       "1  federal appeal court rejected direct constitut...  \n",
       "2  part trump administration effort slow migrant ...  \n",
       "3  president donald trump trump tropic brazilian ...  \n",
       "4  u senator elizabeth warren competing democrati...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 represents most left, 3 represents most right\n",
    "bins = [-41, -5, 5, 41]\n",
    "names = ['1', '2', '3']\n",
    "\n",
    "multi_df = df.loc[:]\n",
    "multi_df['Category'] = pd.cut(multi_df['Bias'], bins, labels=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfVectorizer(max_features = 800)\n",
    "tfidf = tfidf_transformer.fit_transform(multi_df['cleaned_body'])\n",
    "\n",
    "X = pd.DataFrame(tfidf.toarray(), columns=tfidf_transformer.get_feature_names())\n",
    "y = multi_df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = X.columns\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(scaled, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.6133651551312649\n"
     ]
    }
   ],
   "source": [
    "# Defining Model\n",
    "mnb = MultinomialNB()\n",
    "# Training Model\n",
    "mnb.fit(X_train, y_train)\n",
    "# Making Predictions\n",
    "y_pred = mnb.predict(X_test)\n",
    "# Evaluating\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score is:  0.7867350368960674\n"
     ]
    }
   ],
   "source": [
    "y_probs = mnb.predict_proba(X_test)\n",
    "roc_auc = roc_auc_score(y_test, y_probs, average=\"macro\", multi_class=\"ovo\")\n",
    "print('ROC Score is: ', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.6276849642004774\n"
     ]
    }
   ],
   "source": [
    "# Defining Model\n",
    "regressor = LogisticRegression()\n",
    "# Training Model\n",
    "regressor.fit(X_train, y_train)\n",
    "# Making Predictions\n",
    "y_pred2 = regressor.predict(X_test)\n",
    "# Evaluating\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score is:  0.7955378394790632\n"
     ]
    }
   ],
   "source": [
    "y_probs = regressor.predict_proba(X_test)\n",
    "roc_auc = roc_auc_score(y_test, y_probs, average=\"macro\", multi_class=\"ovo\")\n",
    "print('ROC Score is: ', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.6205250596658711\n"
     ]
    }
   ],
   "source": [
    "# Defining Model\n",
    "clf = SVC(probability=True)\n",
    "# Training Model\n",
    "clf.fit(X_train, y_train)\n",
    "# Making Predictions\n",
    "y_pred3 = clf.predict(X_test)\n",
    "# Evaluating\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score is:  0.8277007189446803\n"
     ]
    }
   ],
   "source": [
    "y_probs = clf.predict_proba(X_test)\n",
    "roc_auc = roc_auc_score(y_test, y_probs, average=\"macro\", multi_class=\"ovo\")\n",
    "print('ROC Score is: ', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'pred1': y_pred,\n",
    "                        'pred2': y_pred2,\n",
    "                        'pred3': y_pred3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred3</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pred1 pred2 pred3 final\n",
       "0     2     2     2     2\n",
       "1     2     2     2     2\n",
       "2     2     1     2     2\n",
       "3     1     2     1     1\n",
       "4     2     2     2     2"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['final'] = results.mode(axis=1)[0]\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.6205250596658711\n"
     ]
    }
   ],
   "source": [
    "final_pred = results['final']\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.62      0.60       162\n",
      "           2       0.67      0.69      0.68       165\n",
      "           3       0.57      0.47      0.51        92\n",
      "\n",
      "    accuracy                           0.61       419\n",
      "   macro avg       0.60      0.59      0.60       419\n",
      "weighted avg       0.61      0.61      0.61       419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.64      0.63       162\n",
      "           2       0.65      0.67      0.66       165\n",
      "           3       0.59      0.52      0.55        92\n",
      "\n",
      "    accuracy                           0.63       419\n",
      "   macro avg       0.62      0.61      0.62       419\n",
      "weighted avg       0.63      0.63      0.63       419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.69      0.63       162\n",
      "           2       0.63      0.75      0.68       165\n",
      "           3       0.78      0.27      0.40        92\n",
      "\n",
      "    accuracy                           0.62       419\n",
      "   macro avg       0.67      0.57      0.57       419\n",
      "weighted avg       0.65      0.62      0.60       419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.65      0.62       162\n",
      "           2       0.65      0.73      0.69       165\n",
      "           3       0.62      0.36      0.46        92\n",
      "\n",
      "    accuracy                           0.62       419\n",
      "   macro avg       0.62      0.58      0.59       419\n",
      "weighted avg       0.62      0.62      0.61       419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, final_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using VotingClassifier sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('svm', clf), \n",
    "              ('naivebayes', mnb), \n",
    "              ('logistic', regressor)]\n",
    "\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "results = cross_val_score(ensemble, X, y, cv=kfold, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5791044776119403"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.6276849642004774\n"
     ]
    }
   ],
   "source": [
    "estimators = [('svm', clf), \n",
    "              ('naivebayes', mnb), \n",
    "              ('logistic', regressor)]\n",
    "\n",
    "# Defining Model\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "# Training Model\n",
    "ensemble = ensemble.fit(X_train, y_train)\n",
    "# Making Predictions\n",
    "y_pred4 = ensemble.predict(X_test)\n",
    "# Evaluating\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score is:  0.8227209697620325\n"
     ]
    }
   ],
   "source": [
    "y_probs = ensemble.predict_proba(X_test)\n",
    "roc_auc = roc_auc_score(y_test, y_probs, average=\"macro\", multi_class=\"ovo\")\n",
    "print('ROC Score is: ', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
