{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK sentiment analysis using nlp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/monicabellare/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/monicabellare/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# This file contains functions that helps with NLP processing                             \n",
    "from statistics import mean\n",
    "\n",
    "# Natural Language Processing                                                             \n",
    "### General                                                                               \n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/monicabellare/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Sentiment Analysis                                                                    \n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "### Part of Speech Tagging                                                                \n",
    "# import spacy                                                                            \n",
    "# nlp = spacy.load('en_core_web_sm')  \n",
    "\n",
    "def get_sentiment(text):\n",
    "    # gets the compound score of the sentiment using the VADER lexicon                    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    results = sid.polarity_scores(text)\n",
    "    sentiment = results['compound']\n",
    "    return sentiment\n",
    "\n",
    "def get_entities(text):\n",
    "    # gets the entities from the sentence and returns a list of them                      \n",
    "    doc = nlp(text)\n",
    "    return list(doc.ents)\n",
    "\n",
    "def extract_sentences(word, text):\n",
    "    # extract all sentences in text in which word appears                                 \n",
    "    sentences = [sentence for sentence in text.split('.') if word in sentence]\n",
    "    return sentences\n",
    "\n",
    "def extract_get_sentiment(word, text):\n",
    "    # returns aggregate of sentiment for all sentences that contains word in text         \n",
    "    text = text.lower()\n",
    "    word = word.lower()\n",
    "\n",
    "    sentiments = [get_sentiment(sentence) for sentence in extract_sentences(word, text)]\n",
    "\n",
    "    if len(sentiments) > 1:\n",
    "        return mean(sentiments)\n",
    "    return 0\n",
    "\n",
    "def process_text(text):\n",
    "    # 1. Lowercase text                                                                   \n",
    "    # 2. Removes punctuation                                                              \n",
    "    # 3. Removes digits                                                                   \n",
    "    # 4. Removes stopwords                                                                \n",
    "    # 5. Lemmatizes remaining words                                                       \n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    nopunc_digit = [char for char in text if char not in string.punctuation and not char.isdigit()]\n",
    "    nopunc_digit = ''.join(nopunc_digit)\n",
    "\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmatized = [wnl.lemmatize(word) for word in nopunc_digit.split() if not wnl.lemmatize(word) in set(stopwords.words('english'))]\n",
    "    lemmatized = ' '.join(lemmatized)\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Url</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Header</th>\n",
       "      <th>Body</th>\n",
       "      <th>n_links</th>\n",
       "      <th>Source</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://abcnews.go.com/Politics/abortion-right...</td>\n",
       "      <td>Devin Dwyer</td>\n",
       "      <td>Wed, 17 Apr 2019 10:14:00 GMT</td>\n",
       "      <td>Abortion rights group asks Supreme Court to st...</td>\n",
       "      <td>Abortion rights advocates have asked the U.S. ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>ABC</td>\n",
       "      <td>1.67</td>\n",
       "      <td>49.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://abcnews.go.com/Politics/appeals-court-...</td>\n",
       "      <td>Ali Dukakis</td>\n",
       "      <td>Tue, 26 Feb 2019 09:05:00 GMT</td>\n",
       "      <td>Appeals court says special counsel Robert Muel...</td>\n",
       "      <td>A federal appeals court rejected the most dire...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.67</td>\n",
       "      <td>51.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://abcnews.go.com/Politics/attorney-gener...</td>\n",
       "      <td>Luke Barr</td>\n",
       "      <td>Wed, 17 Apr 2019 14:02:00 GMT</td>\n",
       "      <td>Attorney general orders some asylum seekers to...</td>\n",
       "      <td>As part of the Trump administration's effort t...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ABC</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>43.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://abcnews.go.com/Politics/donald-trump-t...</td>\n",
       "      <td>Meridith McGraw</td>\n",
       "      <td>Tue, 19 Mar 2019 12:44:00 GMT</td>\n",
       "      <td>Donald Trump and 'the Trump of the Tropics,' B...</td>\n",
       "      <td>President Donald Trump and \"the Trump of the T...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>ABC</td>\n",
       "      <td>-4.33</td>\n",
       "      <td>52.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://abcnews.go.com/Politics/electoral-coll...</td>\n",
       "      <td>Matthew Dowd</td>\n",
       "      <td>Tue, 19 Mar 2019 21:39:00 GMT</td>\n",
       "      <td>The Electoral College limits the campaign play...</td>\n",
       "      <td>U.S Senator Elizabeth Warren, who is competing...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>ABC</td>\n",
       "      <td>-10.00</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Url           Author  \\\n",
       "0  https://abcnews.go.com/Politics/abortion-right...      Devin Dwyer   \n",
       "1  https://abcnews.go.com/Politics/appeals-court-...      Ali Dukakis   \n",
       "2  https://abcnews.go.com/Politics/attorney-gener...        Luke Barr   \n",
       "3  https://abcnews.go.com/Politics/donald-trump-t...  Meridith McGraw   \n",
       "4  https://abcnews.go.com/Politics/electoral-coll...     Matthew Dowd   \n",
       "\n",
       "                            Date  \\\n",
       "0  Wed, 17 Apr 2019 10:14:00 GMT   \n",
       "1  Tue, 26 Feb 2019 09:05:00 GMT   \n",
       "2  Wed, 17 Apr 2019 14:02:00 GMT   \n",
       "3  Tue, 19 Mar 2019 12:44:00 GMT   \n",
       "4  Tue, 19 Mar 2019 21:39:00 GMT   \n",
       "\n",
       "                                              Header  \\\n",
       "0  Abortion rights group asks Supreme Court to st...   \n",
       "1  Appeals court says special counsel Robert Muel...   \n",
       "2  Attorney general orders some asylum seekers to...   \n",
       "3  Donald Trump and 'the Trump of the Tropics,' B...   \n",
       "4  The Electoral College limits the campaign play...   \n",
       "\n",
       "                                                Body  n_links Source   Bias  \\\n",
       "0  Abortion rights advocates have asked the U.S. ...      3.0    ABC   1.67   \n",
       "1  A federal appeals court rejected the most dire...      2.0    ABC   0.67   \n",
       "2  As part of the Trump administration's effort t...      6.0    ABC  -2.75   \n",
       "3  President Donald Trump and \"the Trump of the T...     10.0    ABC  -4.33   \n",
       "4  U.S Senator Elizabeth Warren, who is competing...      5.0    ABC -10.00   \n",
       "\n",
       "   Quality  \n",
       "0    49.00  \n",
       "1    51.67  \n",
       "2    43.50  \n",
       "3    52.67  \n",
       "4    32.00  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=['Url','Author','Date','Body','n_links','Source','Bias','Quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Header\n",
      "0     Abortion rights group asks Supreme Court to st...\n",
      "1     Appeals court says special counsel Robert Muel...\n",
      "2     Attorney general orders some asylum seekers to...\n",
      "3     Donald Trump and 'the Trump of the Tropics,' B...\n",
      "4     The Electoral College limits the campaign play...\n",
      "...                                                 ...\n",
      "1670  12 French churches attacked before Notre Dame ...\n",
      "1671  DOJ sued for details of payments to Christophe...\n",
      "1672  Fox News stars pull plug on history of church ...\n",
      "1673    Major U.S. bank shuts down 'alt-right' accounts\n",
      "1674    Schiff launches next front in war against Trump\n",
      "\n",
      "[1675 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_sent = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(header_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,1674):\n",
    "    header_sent.append(get_sentiment(str(x.loc[[i]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5574, 0.4019, 0.0, 0.0, 0.0, 0.0258, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4404, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2263, 0.0, -0.5423, 0.0, 0.0772, 0.0, -0.4404, -0.6249, 0.0, -0.4404, 0.4588, 0.0, 0.2732, -0.1531, -0.4939, 0.0, 0.0, 0.0, -0.4215, 0.0, 0.0, 0.0, 0.2023, 0.0, 0.0, 0.2023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3612, -0.2732, 0.0, 0.0, -0.4215, 0.0, -0.5106, 0.0, -0.4767, 0.0, 0.0516, 0.0, -0.2263, 0.0, 0.128, -0.6124, 0.0, 0.5267, 0.2023, -0.5106, -0.2023, 0.0, 0.2263, 0.0, -0.4939, 0.0, 0.0, 0.0, -0.4019, 0.0, 0.0, 0.0, 0.0, 0.5106, 0.0, 0.0, -0.6249, -0.3818, 0.0, 0.0, 0.0, -0.7096, -0.5106, 0.5574, 0.4019, 0.0, -0.5994, 0.4939, 0.0, 0.0, 0.3612, -0.2732, 0.0, -0.4215, 0.0, -0.5106, 0.0, -0.4767, 0.0, 0.0516, 0.0, -0.2263, 0.0, 0.128, -0.6124, 0.0, 0.5267, 0.2023, -0.5106, -0.2023, 0.0, 0.2263, 0.0, -0.4939, 0.0, 0.0, 0.0, -0.4019, 0.0, 0.0, 0.0, 0.0, 0.5106, 0.0, 0.0, -0.6249, 0.0, 0.0, 0.0, 0.0, -0.7096, -0.5106, 0.5574, 0.4019, 0.6369, -0.5994, 0.4939, 0.0, 0.0, -0.5423, 0.4404, 0.0, -0.0258, 0.0, -0.6705, 0.0, 0.1027, 0.0, -0.4215, -0.1027, 0.0, 0.0, -0.5267, 0.4019, -0.3818, 0.4019, 0.0, -0.0818, -0.2263, 0.0, -0.2023, 0.0, 0.0, -0.34, 0.0, -0.34, -0.296, 0.0, -0.4404, 0.0, 0.0258, 0.0, -0.7783, -0.7579, -0.2732, -0.5574, 0.0, 0.0, -0.6249, -0.296, 0.0, 0.0, 0.4215, -0.2732, -0.6705, 0.0, 0.0, 0.4201, 0.0, 0.5267, -0.128, -0.7003, 0.0, -0.3182, 0.0, 0.0, -0.6808, -0.7096, -0.34, 0.0, -0.3818, 0.0, 0.0, -0.4404, 0.25, -0.3818, 0.0, -0.34, 0.0, 0.0, -0.296, 0.0, 0.0, 0.0, 0.5574, 0.0, 0.0, 0.3818, 0.0516, -0.765, 0.0, 0.0, -0.2263, 0.6369, 0.1027, 0.0, 0.0, -0.2263, -0.296, 0.0, -0.5098, 0.0, 0.0, -0.296, 0.0, -0.4767, -0.296, 0.0, -0.296, -0.34, 0.0, 0.0, 0.0, 0.0, -0.4019, 0.0, 0.0, 0.0, -0.4588, -0.128, 0.0, 0.0, 0.6249, 0.0, -0.34, 0.0, -0.0516, 0.0, 0.0772, 0.0, 0.0, 0.743, 0.3182, 0.0, 0.0, 0.0, 0.0, 0.3612, 0.0, 0.0, 0.0, 0.296, 0.0, 0.0, 0.802, -0.5859, 0.0, 0.0, 0.3182, -0.5719, 0.0, 0.0, 0.1513, 0.5267, 0.0, 0.0, 0.0, 0.128, -0.4588, -0.4767, 0.0, 0.0, -0.1027, 0.0, 0.7717, 0.0, 0.0, 0.0, -0.6486, 0.6666, 0.0, -0.5859, -0.34, 0.0, 0.0, -0.128, -0.6597, 0.0, -0.5766, -0.7579, -0.34, 0.0, 0.0, 0.0, 0.0, -0.128, -0.6908, 0.0, -0.25, 0.0, 0.0, 0.0, 0.0, 0.1027, 0.0, 0.0, 0.0, 0.0772, 0.0, 0.5574, 0.0, 0.0, -0.5859, 0.0, 0.0, 0.0, 0.0, -0.4588, -0.0258, 0.0, -0.25, 0.3612, 0.0, 0.0, 0.3612, 0.0, 0.0, -0.6808, -0.3182, 0.0, 0.0, 0.0, -0.4404, -0.34, 0.0, -0.2732, -0.2732, -0.296, 0.0, 0.0, 0.3612, 0.0, -0.5267, -0.7717, -0.128, 0.3182, 0.0, -0.2732, -0.3182, 0.0, -0.5994, 0.2732, 0.0, 0.0, 0.4404, 0.34, 0.0, 0.3182, -0.4404, 0.0, 0.5574, -0.4215, -0.6249, 0.0, -0.34, 0.0, 0.0, 0.0, 0.0, -0.1027, 0.0, 0.0, 0.5719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0516, 0.0, 0.0, -0.3818, -0.3182, 0.0, 0.0, -0.4019, -0.5106, 0.0, 0.0, -0.3818, 0.0772, -0.6369, 0.0, 0.128, 0.34, -0.2263, 0.128, 0.0, 0.0, -0.5106, 0.0, 0.0, 0.4939, 0.0, 0.8074, 0.0, 0.0, 0.0, 0.0, 0.0, -0.34, 0.0, 0.0, -0.296, 0.5859, 0.0, 0.0, 0.4939, 0.0, 0.0, 0.0, 0.0, 0.0, -0.25, -0.3818, 0.0, 0.6369, 0.0, 0.2023, 0.0772, -0.5423, -0.6597, -0.3818, 0.0258, -0.128, -0.128, 0.0, -0.2023, -0.4404, -0.4404, 0.0, 0.0, 0.0, -0.3612, 0.0, 0.2023, 0.0, 0.6597, -0.4404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3612, 0.0, -0.5106, -0.296, 0.0, 0.4019, -0.5267, 0.0, -0.3818, -0.4754, 0.0, 0.0, 0.0, -0.5106, 0.3818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6124, 0.0, 0.0, 0.0, 0.0, 0.0, -0.7003, -0.5106, 0.0, 0.0, 0.4404, 0.0, 0.5106, 0.0, 0.2732, -0.6249, 0.3595, 0.0, 0.1779, 0.5859, 0.0, -0.3612, 0.0, 0.0, 0.0, 0.0, -0.296, -0.4404, -0.4404, 0.0, 0.0, 0.0, 0.0, 0.0, -0.296, -0.3182, 0.0, 0.0, 0.0, 0.3612, 0.0, 0.0, -0.4588, 0.0, -0.1027, -0.4019, -0.4404, 0.128, 0.7269, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3612, -0.34, -0.3612, -0.5574, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3182, 0.0, 0.0258, 0.4588, 0.3612, 0.0, 0.0, 0.3612, 0.0, 0.0, 0.0, 0.0, -0.4588, 0.0, 0.0, 0.0, 0.0, 0.5719, -0.4019, -0.296, -0.296, -0.8268, -0.1531, 0.0, 0.0, 0.0, 0.128, 0.2732, 0.0, -0.6705, -0.34, 0.0, 0.0, 0.0, 0.0, 0.1531, -0.34, 0.0, 0.0, -0.4019, -0.2411, 0.0, 0.0, 0.4215, 0.0, 0.4404, 0.0, 0.0, 0.0772, 0.4767, 0.2732, -0.4939, 0.0, -0.5859, 0.0, 0.0, 0.0, 0.0, -0.7783, 0.0, -0.8516, 0.0, 0.5859, 0.0, 0.0, 0.0, 0.0, 0.0, -0.296, 0.0, -0.6486, 0.0, 0.0, -0.5574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4215, -0.4404, 0.5574, 0.0, 0.5574, -0.5267, -0.0258, -0.7003, -0.4019, -0.4404, 0.1027, -0.8271, -0.5267, 0.0, 0.25, 0.0, 0.0, -0.296, 0.5106, 0.0, 0.0, -0.7003, -0.6486, 0.0, -0.3818, -0.5574, 0.0, 0.0, 0.0, 0.0, 0.4939, 0.0, 0.0, -0.5106, 0.0, 0.0, -0.296, 0.0, 0.0, -0.4767, 0.0, -0.2732, 0.0, 0.0, -0.4215, 0.0, -0.6597, -0.2263, -0.296, 0.5487, 0.0, 0.0, 0.0, -0.2263, 0.2411, 0.0, 0.0, -0.8271, -0.5859, 0.0, -0.2023, 0.0, 0.0, 0.3089, 0.2263, 0.0, 0.4767, 0.1779, 0.0, 0.5719, -0.5574, -0.296, 0.0, -0.296, -0.1531, -0.2023, 0.0, -0.6486, 0.0, 0.0, 0.4019, -0.5574, -0.8625, 0.0, 0.0, 0.3612, -0.802, -0.34, 0.0, -0.0516, 0.3612, -0.4939, 0.0, 0.0, 0.5267, 0.0, -0.5994, -0.34, 0.0, 0.5859, 0.0, -0.2263, 0.3182, -0.3353, 0.0, 0.0, 0.2732, -0.4767, 0.4767, 0.0, 0.2023, 0.0, 0.4019, -0.5106, -0.1027, 0.3612, 0.0, 0.0, 0.0, 0.5423, -0.3182, 0.128, 0.0, 0.0, -0.6705, 0.0, -0.3182, 0.0, 0.4215, 0.0, -0.5574, -0.34, -0.4939, 0.0, 0.0, -0.5106, 0.0, 0.0, -0.3612, 0.0, -0.296, 0.0, 0.0, 0.0258, -0.296, 0.0, 0.0, -0.2732, -0.4215, 0.0, 0.4215, -0.8126, 0.0, -0.4019, -0.6705, 0.0, 0.0, -0.6908, -0.25, 0.0, 0.0, -0.1027, 0.0, 0.2023, 0.0, -0.3818, 0.0, 0.0, -0.4404, 0.0, 0.0, 0.0, 0.4927, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4215, -0.3612, 0.0, -0.6808, 0.0, 0.0, 0.2263, -0.4939, 0.0, 0.0, 0.0, -0.5574, 0.0, -0.4404, 0.0, -0.34, 0.0, 0.0, 0.4019, 0.0, -0.1027, 0.0, 0.0, 0.34, 0.0, 0.0, 0.0, 0.0, 0.3182, 0.0, 0.0, 0.3182, 0.0, 0.4767, 0.0, 0.34, 0.0, 0.0, 0.0, 0.0, 0.296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.25, -0.5994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3612, 0.0, 0.0, -0.4019, 0.0, -0.6124, 0.0, -0.6249, -0.3818, 0.128, 0.128, 0.0, -0.296, 0.0, 0.0, -0.1779, 0.3182, 0.0, 0.0, 0.0, 0.0, 0.2023, 0.0, 0.0, 0.0, 0.0, -0.5106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2023, -0.3818, -0.3818, -0.6486, 0.0, 0.3818, 0.0, -0.34, 0.0, -0.34, 0.3612, 0.0, -0.4215, 0.0, -0.3818, 0.4019, 0.0, 0.0, -0.4404, -0.8555, 0.0, -0.4404, 0.0, -0.4939, 0.128, 0.0, -0.4767, 0.0, 0.0, 0.5093, 0.0, 0.0, 0.0, 0.2023, 0.0258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0516, 0.0, 0.2263, -0.7269, -0.4404, 0.0, -0.4019, 0.0, 0.2023, 0.0, -0.2263, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4215, 0.0516, 0.0, 0.0, -0.7184, 0.0, 0.0, 0.0, 0.4939, 0.0, 0.0, 0.5267, 0.0, -0.4019, 0.6369, 0.0, -0.5423, 0.0, 0.0, 0.4019, 0.489, 0.0, 0.0, 0.5574, -0.4215, 0.0, 0.0, 0.2584, -0.296, 0.0, 0.0, 0.0, -0.4767, 0.5574, 0.4019, -0.7351, 0.0, -0.4939, 0.0, -0.6908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2263, 0.6808, -0.0772, -0.2263, -0.1027, 0.5574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5267, 0.0, 0.25, 0.0, 0.0, -0.4019, 0.0, 0.0, 0.0, 0.0, -0.4588, -0.3612, 0.0, 0.0, 0.0, -0.3182, -0.6486, 0.128, 0.0, 0.0, 0.4215, 0.0, 0.0, 0.4939, 0.0, 0.7717, -0.4939, 0.0, -0.0258, -0.2023, 0.296, -0.3724, 0.0, 0.6249, 0.3182, 0.0, 0.0, -0.4404, -0.0516, 0.0, 0.3612, -0.5267, -0.34, 0.0, 0.0, -0.2263, 0.0, 0.0, 0.296, -0.1027, 0.0, -0.2263, -0.836, 0.0, -0.2023, -0.3818, 0.0, -0.6249, -0.1027, 0.3612, 0.0258, -0.3818, -0.6705, 0.0, 0.0, 0.0, 0.0, -0.6124, 0.296, -0.6808, 0.296, 0.0, -0.4588, 0.0, -0.6486, -0.1027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3818, 0.0, 0.0, 0.0, 0.4404, -0.296, 0.0, 0.3612, -0.4939, -0.5574, 0.5574, 0.0, 0.5106, 0.0, 0.0, 0.0772, 0.3182, 0.0, 0.0, 0.0, 0.0, -0.5106, 0.5859, 0.0, 0.0, -0.296, 0.1779, 0.0, -0.34, -0.4939, -0.836, -0.6249, 0.0, 0.0, -0.4019, -0.4404, 0.0, 0.0, 0.3182, 0.0, -0.4767, 0.0, 0.0, 0.0, 0.0, 0.0258, 0.802, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.7096, -0.5423, -0.6705, 0.0, 0.2023, -0.128, 0.0, 0.0, -0.4939, 0.0, -0.3818, -0.1779, 0.0772, -0.34, 0.0, 0.0, 0.0, 0.0, 0.0772, -0.296, 0.0, -0.0516, -0.128, 0.0, 0.4019, 0.5574, 0.8225, 0.5267, 0.0, 0.4588, 0.0, 0.2732, 0.0, 0.0, -0.7717, 0.0, 0.0, 0.0, -0.4404, 0.0, 0.0, 0.4389, 0.128, -0.25, -0.34, -0.4404, 0.0, 0.5267, 0.4019, -0.3818, 0.0, 0.0, 0.0, -0.6124, -0.2263, 0.0, -0.5423, 0.4404, -0.2263, 0.5994, 0.4215, -0.2263, 0.3818, -0.3818, 0.0, -0.3818, 0.0, -0.25, -0.4404, -0.34, 0.4767, 0.0, -0.4588, 0.0, -0.0258, 0.0, 0.0, -0.1027, -0.4767, -0.2263, 0.0, 0.2263, 0.0, 0.4404, 0.0, 0.0, 0.0, 0.0, -0.2023, 0.5267, 0.2732, 0.0, 0.2732, 0.0, -0.5994, -0.765, -0.296, -0.4588, 0.7906, -0.3818, -0.3412, 0.0, 0.0, 0.0, 0.0, -0.765, 0.0, 0.0, 0.0, 0.0, -0.2411, 0.0, 0.0, 0.0, -0.6249, -0.4404, -0.296, 0.0, 0.5574, 0.0, 0.0, 0.0, 0.0, -0.25, -0.3818, 0.0, 0.5574, -0.3818, -0.6249, -0.25, 0.0, 0.1531, -0.5859, 0.0, 0.0, 0.0, 0.0, 0.5574, 0.5574, 0.0, 0.0, 0.0, 0.4939, 0.0, 0.0, 0.0, -0.25, 0.0, 0.3818, -0.4215, -0.4767, -0.0191, -0.7003, -0.4588, -0.5267, 0.0, 0.3612, 0.4767, 0.2023, 0.5574, 0.4215, 0.0, -0.25, -0.4404, 0.0, -0.1027, 0.0, 0.0, 0.0, -0.5994, 0.0, -0.3612, 0.0, 0.0, 0.2023, 0.0, 0.0, 0.5994, -0.296, 0.0, -0.3818, -0.2732, -0.4215, 0.0, -0.7783, -0.9552, 0.0, 0.0, 0.0, -0.4404, -0.3612, -0.4003, 0.5574, 0.2577, 0.4559, 0.0, 0.0, 0.0, 0.5514, 0.0, -0.0258, -0.6705, 0.0, 0.0, -0.4939, -0.2023, 0.0, 0.0, 0.0, -0.34, 0.0, -0.1779, 0.6759, -0.6124, 0.4019, 0.0, 0.0, -0.2732, -0.5423, 0.0, -0.4215, 0.0, 0.0, -0.25, 0.0258, 0.0, 0.0, 0.0, -0.296, -0.34, 0.0, 0.0, 0.0, 0.0, 0.0, 0.128, 0.5106, -0.4588, 0.6124, -0.5859, 0.5106, 0.4019, 0.5994, 0.7096, 0.0, -0.6124, 0.0, 0.0, 0.3182, 0.4588, -0.2732, -0.6124, 0.765, -0.4404, -0.296, 0.0, -0.4588, 0.0, 0.128, 0.0258, -0.3818, 0.5574, 0.0, 0.296, 0.0, 0.0, -0.3612, 0.0, 0.2748, 0.0, 0.3818, 0.0, -0.3182, 0.4019, 0.4404, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4939, 0.0772, 0.0, -0.6369, 0.0, 0.4019, 0.0, 0.0, 0.0, -0.6597, 0.0, 0.0, 0.1779, 0.0, 0.0, 0.0, -0.6249, 0.0, 0.0516, -0.3612, -0.0516, 0.0, 0.0, -0.4215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7351, 0.4215, 0.0, 0.0, 0.0, -0.6808, -0.4767, 0.0, 0.1326, 0.0, -0.34, 0.0, -0.2263, -0.4019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3612, -0.4404, 0.0516, -0.3818, 0.0, 0.0, 0.0, 0.0, 0.2023, 0.0, 0.0, 0.7506, 0.6705, -0.6486, 0.0, 0.0, 0.1027, -0.34, 0.8126, 0.0, 0.0, -0.296, -0.7184, 0.0, 0.0, -0.1531, 0.0, -0.296, 0.4215, -0.5106, 0.0, 0.4939, -0.5423, 0.6124, 0.0, -0.3612, 0.0, 0.0, -0.2263, 0.7269, -0.296, 0.0, -0.4767, -0.3182, 0.0, 0.3612, 0.0, 0.7964, 0.0, 0.0, 0.0, 0.34, 0.0, 0.0, 0.0, 0.34, -0.7351, -0.34, 0.0, 0.0, -0.6486, 0.0, -0.6249, 0.0, 0.6239, 0.5859, 0.2023, -0.0258, 0.2732, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1027, 0.0, 0.2732, 0.0, 0.0, -0.128, -0.5574, -0.4019, -0.0772, 0.0, 0.0, 0.0, 0.4019, 0.0, -0.3182, 0.0258, -0.4003, 0.0, 0.0, 0.0, 0.0, -0.3818, 0.0, 0.0, -0.1027, 0.0, -0.6486, 0.0, -0.2732, 0.0, -0.3818, 0.2732, 0.0, 0.0, 0.0, -0.7717, 0.0, 0.0, -0.4215, 0.0, 0.0, 0.0, 0.0, -0.5574, 0.0, 0.0, 0.0, 0.5423, -0.4404, 0.0, 0.0, 0.0516, -0.34, 0.0, 0.0, -0.25, 0.5106, 0.0, 0.0, 0.2732, -0.1197, 0.0, -0.5423, 0.0, -0.0943, 0.0, 0.0, 0.0, 0.0, -0.8715, -0.34, -0.7269, 0.0, 0.4019, -0.296, 0.0, 0.0, 0.0, -0.6908, 0.5859, -0.2263, -0.6249, -0.4019, -0.4767, -0.4588, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(header_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew = df.rename(columns={'Bias': 'Inital_Bias'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5574, 0.4019, 0.0, 0.0, 0.0, 0.0258, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4404, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2263, 0.0, -0.5423, 0.0, 0.0772, 0.0, -0.4404, -0.6249, 0.0, -0.4404, 0.4588, 0.0, 0.2732, -0.1531, -0.4939, 0.0, 0.0, 0.0, -0.4215, 0.0, 0.0, 0.0, 0.2023, 0.0, 0.0, 0.2023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3612, -0.2732, 0.0, 0.0, -0.4215, 0.0, -0.5106, 0.0, -0.4767, 0.0, 0.0516, 0.0, -0.2263, 0.0, 0.128, -0.6124, 0.0, 0.5267, 0.2023, -0.5106, -0.2023, 0.0, 0.2263, 0.0, -0.4939, 0.0, 0.0, 0.0, -0.4019, 0.0, 0.0, 0.0, 0.0, 0.5106, 0.0, 0.0, -0.6249, -0.3818, 0.0, 0.0, 0.0, -0.7096, -0.5106, 0.5574, 0.4019, 0.0, -0.5994, 0.4939, 0.0, 0.0, 0.3612, -0.2732, 0.0, -0.4215, 0.0, -0.5106, 0.0, -0.4767, 0.0, 0.0516, 0.0, -0.2263, 0.0, 0.128, -0.6124, 0.0, 0.5267, 0.2023, -0.5106, -0.2023, 0.0, 0.2263, 0.0, -0.4939, 0.0, 0.0, 0.0, -0.4019, 0.0, 0.0, 0.0, 0.0, 0.5106, 0.0, 0.0, -0.6249, 0.0, 0.0, 0.0, 0.0, -0.7096, -0.5106, 0.5574, 0.4019, 0.6369, -0.5994, 0.4939, 0.0, 0.0, -0.5423, 0.4404, 0.0, -0.0258, 0.0, -0.6705, 0.0, 0.1027, 0.0, -0.4215, -0.1027, 0.0, 0.0, -0.5267, 0.4019, -0.3818, 0.4019, 0.0, -0.0818, -0.2263, 0.0, -0.2023, 0.0, 0.0, -0.34, 0.0, -0.34, -0.296, 0.0, -0.4404, 0.0, 0.0258, 0.0, -0.7783, -0.7579, -0.2732, -0.5574, 0.0, 0.0, -0.6249, -0.296, 0.0, 0.0, 0.4215, -0.2732, -0.6705, 0.0, 0.0, 0.4201, 0.0, 0.5267, -0.128, -0.7003, 0.0, -0.3182, 0.0, 0.0, -0.6808, -0.7096, -0.34, 0.0, -0.3818, 0.0, 0.0, -0.4404, 0.25, -0.3818, 0.0, -0.34, 0.0, 0.0, -0.296, 0.0, 0.0, 0.0, 0.5574, 0.0, 0.0, 0.3818, 0.0516, -0.765, 0.0, 0.0, -0.2263, 0.6369, 0.1027, 0.0, 0.0, -0.2263, -0.296, 0.0, -0.5098, 0.0, 0.0, -0.296, 0.0, -0.4767, -0.296, 0.0, -0.296, -0.34, 0.0, 0.0, 0.0, 0.0, -0.4019, 0.0, 0.0, 0.0, -0.4588, -0.128, 0.0, 0.0, 0.6249, 0.0, -0.34, 0.0, -0.0516, 0.0, 0.0772, 0.0, 0.0, 0.743, 0.3182, 0.0, 0.0, 0.0, 0.0, 0.3612, 0.0, 0.0, 0.0, 0.296, 0.0, 0.0, 0.802, -0.5859, 0.0, 0.0, 0.3182, -0.5719, 0.0, 0.0, 0.1513, 0.5267, 0.0, 0.0, 0.0, 0.128, -0.4588, -0.4767, 0.0, 0.0, -0.1027, 0.0, 0.7717, 0.0, 0.0, 0.0, -0.6486, 0.6666, 0.0, -0.5859, -0.34, 0.0, 0.0, -0.128, -0.6597, 0.0, -0.5766, -0.7579, -0.34, 0.0, 0.0, 0.0, 0.0, -0.128, -0.6908, 0.0, -0.25, 0.0, 0.0, 0.0, 0.0, 0.1027, 0.0, 0.0, 0.0, 0.0772, 0.0, 0.5574, 0.0, 0.0, -0.5859, 0.0, 0.0, 0.0, 0.0, -0.4588, -0.0258, 0.0, -0.25, 0.3612, 0.0, 0.0, 0.3612, 0.0, 0.0, -0.6808, -0.3182, 0.0, 0.0, 0.0, -0.4404, -0.34, 0.0, -0.2732, -0.2732, -0.296, 0.0, 0.0, 0.3612, 0.0, -0.5267, -0.7717, -0.128, 0.3182, 0.0, -0.2732, -0.3182, 0.0, -0.5994, 0.2732, 0.0, 0.0, 0.4404, 0.34, 0.0, 0.3182, -0.4404, 0.0, 0.5574, -0.4215, -0.6249, 0.0, -0.34, 0.0, 0.0, 0.0, 0.0, -0.1027, 0.0, 0.0, 0.5719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0516, 0.0, 0.0, -0.3818, -0.3182, 0.0, 0.0, -0.4019, -0.5106, 0.0, 0.0, -0.3818, 0.0772, -0.6369, 0.0, 0.128, 0.34, -0.2263, 0.128, 0.0, 0.0, -0.5106, 0.0, 0.0, 0.4939, 0.0, 0.8074, 0.0, 0.0, 0.0, 0.0, 0.0, -0.34, 0.0, 0.0, -0.296, 0.5859, 0.0, 0.0, 0.4939, 0.0, 0.0, 0.0, 0.0, 0.0, -0.25, -0.3818, 0.0, 0.6369, 0.0, 0.2023, 0.0772, -0.5423, -0.6597, -0.3818, 0.0258, -0.128, -0.128, 0.0, -0.2023, -0.4404, -0.4404, 0.0, 0.0, 0.0, -0.3612, 0.0, 0.2023, 0.0, 0.6597, -0.4404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3612, 0.0, -0.5106, -0.296, 0.0, 0.4019, -0.5267, 0.0, -0.3818, -0.4754, 0.0, 0.0, 0.0, -0.5106, 0.3818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6124, 0.0, 0.0, 0.0, 0.0, 0.0, -0.7003, -0.5106, 0.0, 0.0, 0.4404, 0.0, 0.5106, 0.0, 0.2732, -0.6249, 0.3595, 0.0, 0.1779, 0.5859, 0.0, -0.3612, 0.0, 0.0, 0.0, 0.0, -0.296, -0.4404, -0.4404, 0.0, 0.0, 0.0, 0.0, 0.0, -0.296, -0.3182, 0.0, 0.0, 0.0, 0.3612, 0.0, 0.0, -0.4588, 0.0, -0.1027, -0.4019, -0.4404, 0.128, 0.7269, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3612, -0.34, -0.3612, -0.5574, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3182, 0.0, 0.0258, 0.4588, 0.3612, 0.0, 0.0, 0.3612, 0.0, 0.0, 0.0, 0.0, -0.4588, 0.0, 0.0, 0.0, 0.0, 0.5719, -0.4019, -0.296, -0.296, -0.8268, -0.1531, 0.0, 0.0, 0.0, 0.128, 0.2732, 0.0, -0.6705, -0.34, 0.0, 0.0, 0.0, 0.0, 0.1531, -0.34, 0.0, 0.0, -0.4019, -0.2411, 0.0, 0.0, 0.4215, 0.0, 0.4404, 0.0, 0.0, 0.0772, 0.4767, 0.2732, -0.4939, 0.0, -0.5859, 0.0, 0.0, 0.0, 0.0, -0.7783, 0.0, -0.8516, 0.0, 0.5859, 0.0, 0.0, 0.0, 0.0, 0.0, -0.296, 0.0, -0.6486, 0.0, 0.0, -0.5574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4215, -0.4404, 0.5574, 0.0, 0.5574, -0.5267, -0.0258, -0.7003, -0.4019, -0.4404, 0.1027, -0.8271, -0.5267, 0.0, 0.25, 0.0, 0.0, -0.296, 0.5106, 0.0, 0.0, -0.7003, -0.6486, 0.0, -0.3818, -0.5574, 0.0, 0.0, 0.0, 0.0, 0.4939, 0.0, 0.0, -0.5106, 0.0, 0.0, -0.296, 0.0, 0.0, -0.4767, 0.0, -0.2732, 0.0, 0.0, -0.4215, 0.0, -0.6597, -0.2263, -0.296, 0.5487, 0.0, 0.0, 0.0, -0.2263, 0.2411, 0.0, 0.0, -0.8271, -0.5859, 0.0, -0.2023, 0.0, 0.0, 0.3089, 0.2263, 0.0, 0.4767, 0.1779, 0.0, 0.5719, -0.5574, -0.296, 0.0, -0.296, -0.1531, -0.2023, 0.0, -0.6486, 0.0, 0.0, 0.4019, -0.5574, -0.8625, 0.0, 0.0, 0.3612, -0.802, -0.34, 0.0, -0.0516, 0.3612, -0.4939, 0.0, 0.0, 0.5267, 0.0, -0.5994, -0.34, 0.0, 0.5859, 0.0, -0.2263, 0.3182, -0.3353, 0.0, 0.0, 0.2732, -0.4767, 0.4767, 0.0, 0.2023, 0.0, 0.4019, -0.5106, -0.1027, 0.3612, 0.0, 0.0, 0.0, 0.5423, -0.3182, 0.128, 0.0, 0.0, -0.6705, 0.0, -0.3182, 0.0, 0.4215, 0.0, -0.5574, -0.34, -0.4939, 0.0, 0.0, -0.5106, 0.0, 0.0, -0.3612, 0.0, -0.296, 0.0, 0.0, 0.0258, -0.296, 0.0, 0.0, -0.2732, -0.4215, 0.0, 0.4215, -0.8126, 0.0, -0.4019, -0.6705, 0.0, 0.0, -0.6908, -0.25, 0.0, 0.0, -0.1027, 0.0, 0.2023, 0.0, -0.3818, 0.0, 0.0, -0.4404, 0.0, 0.0, 0.0, 0.4927, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4215, -0.3612, 0.0, -0.6808, 0.0, 0.0, 0.2263, -0.4939, 0.0, 0.0, 0.0, -0.5574, 0.0, -0.4404, 0.0, -0.34, 0.0, 0.0, 0.4019, 0.0, -0.1027, 0.0, 0.0, 0.34, 0.0, 0.0, 0.0, 0.0, 0.3182, 0.0, 0.0, 0.3182, 0.0, 0.4767, 0.0, 0.34, 0.0, 0.0, 0.0, 0.0, 0.296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.25, -0.5994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3612, 0.0, 0.0, -0.4019, 0.0, -0.6124, 0.0, -0.6249, -0.3818, 0.128, 0.128, 0.0, -0.296, 0.0, 0.0, -0.1779, 0.3182, 0.0, 0.0, 0.0, 0.0, 0.2023, 0.0, 0.0, 0.0, 0.0, -0.5106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2023, -0.3818, -0.3818, -0.6486, 0.0, 0.3818, 0.0, -0.34, 0.0, -0.34, 0.3612, 0.0, -0.4215, 0.0, -0.3818, 0.4019, 0.0, 0.0, -0.4404, -0.8555, 0.0, -0.4404, 0.0, -0.4939, 0.128, 0.0, -0.4767, 0.0, 0.0, 0.5093, 0.0, 0.0, 0.0, 0.2023, 0.0258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0516, 0.0, 0.2263, -0.7269, -0.4404, 0.0, -0.4019, 0.0, 0.2023, 0.0, -0.2263, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4215, 0.0516, 0.0, 0.0, -0.7184, 0.0, 0.0, 0.0, 0.4939, 0.0, 0.0, 0.5267, 0.0, -0.4019, 0.6369, 0.0, -0.5423, 0.0, 0.0, 0.4019, 0.489, 0.0, 0.0, 0.5574, -0.4215, 0.0, 0.0, 0.2584, -0.296, 0.0, 0.0, 0.0, -0.4767, 0.5574, 0.4019, -0.7351, 0.0, -0.4939, 0.0, -0.6908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2263, 0.6808, -0.0772, -0.2263, -0.1027, 0.5574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5267, 0.0, 0.25, 0.0, 0.0, -0.4019, 0.0, 0.0, 0.0, 0.0, -0.4588, -0.3612, 0.0, 0.0, 0.0, -0.3182, -0.6486, 0.128, 0.0, 0.0, 0.4215, 0.0, 0.0, 0.4939, 0.0, 0.7717, -0.4939, 0.0, -0.0258, -0.2023, 0.296, -0.3724, 0.0, 0.6249, 0.3182, 0.0, 0.0, -0.4404, -0.0516, 0.0, 0.3612, -0.5267, -0.34, 0.0, 0.0, -0.2263, 0.0, 0.0, 0.296, -0.1027, 0.0, -0.2263, -0.836, 0.0, -0.2023, -0.3818, 0.0, -0.6249, -0.1027, 0.3612, 0.0258, -0.3818, -0.6705, 0.0, 0.0, 0.0, 0.0, -0.6124, 0.296, -0.6808, 0.296, 0.0, -0.4588, 0.0, -0.6486, -0.1027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3818, 0.0, 0.0, 0.0, 0.4404, -0.296, 0.0, 0.3612, -0.4939, -0.5574, 0.5574, 0.0, 0.5106, 0.0, 0.0, 0.0772, 0.3182, 0.0, 0.0, 0.0, 0.0, -0.5106, 0.5859, 0.0, 0.0, -0.296, 0.1779, 0.0, -0.34, -0.4939, -0.836, -0.6249, 0.0, 0.0, -0.4019, -0.4404, 0.0, 0.0, 0.3182, 0.0, -0.4767, 0.0, 0.0, 0.0, 0.0, 0.0258, 0.802, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.7096, -0.5423, -0.6705, 0.0, 0.2023, -0.128, 0.0, 0.0, -0.4939, 0.0, -0.3818, -0.1779, 0.0772, -0.34, 0.0, 0.0, 0.0, 0.0, 0.0772, -0.296, 0.0, -0.0516, -0.128, 0.0, 0.4019, 0.5574, 0.8225, 0.5267, 0.0, 0.4588, 0.0, 0.2732, 0.0, 0.0, -0.7717, 0.0, 0.0, 0.0, -0.4404, 0.0, 0.0, 0.4389, 0.128, -0.25, -0.34, -0.4404, 0.0, 0.5267, 0.4019, -0.3818, 0.0, 0.0, 0.0, -0.6124, -0.2263, 0.0, -0.5423, 0.4404, -0.2263, 0.5994, 0.4215, -0.2263, 0.3818, -0.3818, 0.0, -0.3818, 0.0, -0.25, -0.4404, -0.34, 0.4767, 0.0, -0.4588, 0.0, -0.0258, 0.0, 0.0, -0.1027, -0.4767, -0.2263, 0.0, 0.2263, 0.0, 0.4404, 0.0, 0.0, 0.0, 0.0, -0.2023, 0.5267, 0.2732, 0.0, 0.2732, 0.0, -0.5994, -0.765, -0.296, -0.4588, 0.7906, -0.3818, -0.3412, 0.0, 0.0, 0.0, 0.0, -0.765, 0.0, 0.0, 0.0, 0.0, -0.2411, 0.0, 0.0, 0.0, -0.6249, -0.4404, -0.296, 0.0, 0.5574, 0.0, 0.0, 0.0, 0.0, -0.25, -0.3818, 0.0, 0.5574, -0.3818, -0.6249, -0.25, 0.0, 0.1531, -0.5859, 0.0, 0.0, 0.0, 0.0, 0.5574, 0.5574, 0.0, 0.0, 0.0, 0.4939, 0.0, 0.0, 0.0, -0.25, 0.0, 0.3818, -0.4215, -0.4767, -0.0191, -0.7003, -0.4588, -0.5267, 0.0, 0.3612, 0.4767, 0.2023, 0.5574, 0.4215, 0.0, -0.25, -0.4404, 0.0, -0.1027, 0.0, 0.0, 0.0, -0.5994, 0.0, -0.3612, 0.0, 0.0, 0.2023, 0.0, 0.0, 0.5994, -0.296, 0.0, -0.3818, -0.2732, -0.4215, 0.0, -0.7783, -0.9552, 0.0, 0.0, 0.0, -0.4404, -0.3612, -0.4003, 0.5574, 0.2577, 0.4559, 0.0, 0.0, 0.0, 0.5514, 0.0, -0.0258, -0.6705, 0.0, 0.0, -0.4939, -0.2023, 0.0, 0.0, 0.0, -0.34, 0.0, -0.1779, 0.6759, -0.6124, 0.4019, 0.0, 0.0, -0.2732, -0.5423, 0.0, -0.4215, 0.0, 0.0, -0.25, 0.0258, 0.0, 0.0, 0.0, -0.296, -0.34, 0.0, 0.0, 0.0, 0.0, 0.0, 0.128, 0.5106, -0.4588, 0.6124, -0.5859, 0.5106, 0.4019, 0.5994, 0.7096, 0.0, -0.6124, 0.0, 0.0, 0.3182, 0.4588, -0.2732, -0.6124, 0.765, -0.4404, -0.296, 0.0, -0.4588, 0.0, 0.128, 0.0258, -0.3818, 0.5574, 0.0, 0.296, 0.0, 0.0, -0.3612, 0.0, 0.2748, 0.0, 0.3818, 0.0, -0.3182, 0.4019, 0.4404, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4939, 0.0772, 0.0, -0.6369, 0.0, 0.4019, 0.0, 0.0, 0.0, -0.6597, 0.0, 0.0, 0.1779, 0.0, 0.0, 0.0, -0.6249, 0.0, 0.0516, -0.3612, -0.0516, 0.0, 0.0, -0.4215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7351, 0.4215, 0.0, 0.0, 0.0, -0.6808, -0.4767, 0.0, 0.1326, 0.0, -0.34, 0.0, -0.2263, -0.4019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3612, -0.4404, 0.0516, -0.3818, 0.0, 0.0, 0.0, 0.0, 0.2023, 0.0, 0.0, 0.7506, 0.6705, -0.6486, 0.0, 0.0, 0.1027, -0.34, 0.8126, 0.0, 0.0, -0.296, -0.7184, 0.0, 0.0, -0.1531, 0.0, -0.296, 0.4215, -0.5106, 0.0, 0.4939, -0.5423, 0.6124, 0.0, -0.3612, 0.0, 0.0, -0.2263, 0.7269, -0.296, 0.0, -0.4767, -0.3182, 0.0, 0.3612, 0.0, 0.7964, 0.0, 0.0, 0.0, 0.34, 0.0, 0.0, 0.0, 0.34, -0.7351, -0.34, 0.0, 0.0, -0.6486, 0.0, -0.6249, 0.0, 0.6239, 0.5859, 0.2023, -0.0258, 0.2732, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1027, 0.0, 0.2732, 0.0, 0.0, -0.128, -0.5574, -0.4019, -0.0772, 0.0, 0.0, 0.0, 0.4019, 0.0, -0.3182, 0.0258, -0.4003, 0.0, 0.0, 0.0, 0.0, -0.3818, 0.0, 0.0, -0.1027, 0.0, -0.6486, 0.0, -0.2732, 0.0, -0.3818, 0.2732, 0.0, 0.0, 0.0, -0.7717, 0.0, 0.0, -0.4215, 0.0, 0.0, 0.0, 0.0, -0.5574, 0.0, 0.0, 0.0, 0.5423, -0.4404, 0.0, 0.0, 0.0516, -0.34, 0.0, 0.0, -0.25, 0.5106, 0.0, 0.0, 0.2732, -0.1197, 0.0, -0.5423, 0.0, -0.0943, 0.0, 0.0, 0.0, 0.0, -0.8715, -0.34, -0.7269, 0.0, 0.4019, -0.296, 0.0, 0.0, 0.0, -0.6908, 0.5859, -0.2263, -0.6249, -0.4019, -0.4767, -0.4588, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(list(header_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(0,1674):\n",
    "    dfnew['Header_Sent'][i] = header_sent[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dfnew['Header_Sent'][1674] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 represents most left, 3 represents most right\n",
    "bins = [-1, -0.5, 0.5, 1]\n",
    "names = ['1', '2', '3']\n",
    "\n",
    "multi_df = dfnew.loc[:]\n",
    "multi_df['Category'] = pd.cut(multi_df['Header_Sent'], bins, labels=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfVectorizer(max_features = 800)\n",
    "tfidf = tfidf_transformer.fit_transform(multi_df['Header'])\n",
    "\n",
    "X = pd.DataFrame(tfidf.toarray(), columns=tfidf_transformer.get_feature_names())\n",
    "y = multi_df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = X.columns\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(scaled, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8448687350835322\n"
     ]
    }
   ],
   "source": [
    "# Defining Model\n",
    "mnb = MultinomialNB()\n",
    "# Training Model\n",
    "mnb.fit(X_train, y_train)\n",
    "# Making Predictions\n",
    "y_pred = mnb.predict(X_test)\n",
    "# Evaluating\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score is:  0.6528944911297853\n"
     ]
    }
   ],
   "source": [
    "y_probs = mnb.predict_proba(X_test)\n",
    "roc_auc = roc_auc_score(y_test, y_probs, average=\"macro\", multi_class=\"ovo\")\n",
    "print('ROC Score is: ', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8568019093078759\n"
     ]
    }
   ],
   "source": [
    "# Defining Model\n",
    "regressor = LogisticRegression()\n",
    "# Training Model\n",
    "regressor.fit(X_train, y_train)\n",
    "# Making Predictions\n",
    "y_pred2 = regressor.predict(X_test)\n",
    "# Evaluating\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score is:  0.6947742865262047\n"
     ]
    }
   ],
   "source": [
    "y_probs = regressor.predict_proba(X_test)\n",
    "roc_auc = roc_auc_score(y_test, y_probs, average=\"macro\", multi_class=\"ovo\")\n",
    "print('ROC Score is: ', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8520286396181385\n"
     ]
    }
   ],
   "source": [
    "# Defining Model\n",
    "clf = SVC(probability=True)\n",
    "# Training Model\n",
    "clf.fit(X_train, y_train)\n",
    "# Making Predictions\n",
    "y_pred3 = clf.predict(X_test)\n",
    "# Evaluating\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score is:  0.7005750669833151\n"
     ]
    }
   ],
   "source": [
    "y_probs = clf.predict_proba(X_test)\n",
    "roc_auc = roc_auc_score(y_test, y_probs, average=\"macro\", multi_class=\"ovo\")\n",
    "print('ROC Score is: ', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'pred1': y_pred,\n",
    "                        'pred2': y_pred2,\n",
    "                        'pred3': y_pred3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred3</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pred1 pred2 pred3 final\n",
       "0     2     2     2     2\n",
       "1     2     2     2     2\n",
       "2     2     2     2     2\n",
       "3     2     2     2     2\n",
       "4     2     2     2     2"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['final'] = results.mode(axis=1)[0]\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8544152744630071\n"
     ]
    }
   ],
   "source": [
    "final_pred = results['final']\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.15      0.24        46\n",
      "           2       0.87      0.96      0.91       357\n",
      "           3       0.30      0.19      0.23        16\n",
      "\n",
      "    accuracy                           0.84       419\n",
      "   macro avg       0.57      0.43      0.46       419\n",
      "weighted avg       0.81      0.84      0.81       419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.13      0.21        46\n",
      "           2       0.87      0.98      0.92       357\n",
      "           3       0.50      0.25      0.33        16\n",
      "\n",
      "    accuracy                           0.86       419\n",
      "   macro avg       0.66      0.45      0.49       419\n",
      "weighted avg       0.83      0.86      0.82       419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
